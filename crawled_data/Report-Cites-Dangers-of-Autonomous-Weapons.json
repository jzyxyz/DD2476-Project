{"title": "Report Cites Dangers of Autonomous Weapons", "content": "A new report written by a former Pentagon official who helped establish United States policy on autonomous weapons argues that such weapons could be uncontrollable in real-world environments where they are subject to design failure as well as hacking, spoofing and manipulation by adversaries.\nIn recent years, low-cost sensors and new artificial intelligence technologies have made it increasingly practical to design weapons systems that make killing decisions without human intervention. The specter of so-called killer robots has touched off an international protest movement and a debate within the United Nations about limiting the development and deployment of such systems.\nThe new report was written by Paul Scharre, who directs a program on the future of warfare at the Center for a New American Security, a policy research group in Washington, D.C. From 2008 to 2013, Mr. Scharre worked in the office of the Secretary of Defense, where he helped establish United States policy on unmanned and autonomous weapons. He was one of the authors of a 2012 Defense Department directive that set military policy on the use of such systems.\nIn the report, titled \u201cAutonomous Weapons and Operational Risk,\u201d set to be published on Monday, Mr. Scharre warns about a range of real-world risks associated with weapons systems that are completely autonomous.\nThe report contrasts these completely automated systems, which have the ability to target and kill without human intervention, to weapons that keep humans \u201cin the loop\u201d in the process of selecting and engaging targets.\nMr. Scharre, who served as an Army Ranger in Iraq and Afghanistan, focuses on the potential types of failures that might occur in completely automated systems, as opposed to the way such weapons are intended to work. To underscore the military consequences of technological failures, the report enumerates a history of the types of failures that have occurred in military and commercial systems that are highly automated.\n\u201cAnyone who has ever been frustrated with an automated telephone call support helpline, an alarm clock mistakenly set to \u2018p.m.\u2019 instead of \u2018a.m.,\u2019 or any of the countless frustrations that come with interacting with computers, has experienced the problem of \u2018brittleness\u2019 that plagues automated systems,\u201d Mr. Scharre writes.\nHis underlying point is that autonomous weapons systems will inevitably lack the flexibility that humans have to adapt to novel circumstances and that as a result killing machines will make mistakes that humans would presumably avoid.\nCompletely autonomous weapons are beginning to appear in military arsenals. For example, South Korea has deployed an automated sentry gun along the demilitarized zone with North Korea, and Israel operates a drone aircraft that will attack enemy radar systems when they are detected.\nThe United States military does not have advanced autonomous weapons in its arsenal. However, this year the Defense Department requested almost $1 billion to manufacture Lockheed Martin\u2019s Long Range Anti-Ship Missile, which is described as a \u201csemiautonomous\u201d weapon by the definitions established by the Pentagon\u2019s 2012 memorandum.\nThe missile is controversial because, although a human operator will initially select a target, it is designed to fly for several hundred miles while out of contact with the controller and then automatically identify and attack an enemy ship.\nThe Center for a New American Security report focuses on a range of unexpected behavior in highly computerized systems like system failures and bugs, as well as unanticipated interactions with the environment.\n\u201cOn their first deployment to the Pacific, eight F-22 fighter jets experienced a Y2K-like total computer failure when crossing the international date line,\u201d the report states. \u201cAll onboard computer systems shut down, and the result was nearly a catastrophic loss of the aircraft. While the existence of the international date line could clearly be anticipated, the interaction of the date line with the software was not identified in testing.\u201d\nThe lack of transparency in artificial intelligence technologies that are associated with most recent advances in machine vision and speech recognition systems is also cited as a source of potential catastrophic failures.\nAs an alternative to completely autonomous weapons, the report advocates what it describes as \u201cCentaur Warfighting.\u201d The term \u201ccentaur\u201d has recently come to describe systems that tightly integrate humans and computers. In chess today, teams that combine human experts with artificial intelligence programs dominate in competitions against teams that use only artificial intelligence.\nHowever, in a telephone interview Mr. Scharre acknowledged that simply having a human push the buttons in a weapons system is not enough.\n\u201cHaving a person in the loop is not enough,\u201d he said. \u201cThey can\u2019t be just a cog in the loop. The human has to be actively engaged.\u201d", "date": "Feb. 28, 2016", "href": "https://www.nytimes.com/2016/02/29/technology/report-cites-dangers-of-autonomous-weapons.html", "tags": "cites report autonomous scharre dangers \u201d human automated systems new weapons"}