{"title": "Social media: How can governments regulate it?", "content": "The government has proposed measures to regulate social media companies over harmful content, including \"substantial\" fines and the ability to block services that do not stick to the rules.\nIt will run a consultation until 1 July on plans to create a legal \"duty of care towards users\", overseen by an independent regulator.\nAt the moment, when it comes to graphic content, social media largely relies on self-governance. Sites such as YouTube and Facebook have their own rules about what is unacceptable and the way that users are expected to behave towards one another. \nThis includes content that promotes fake news, hate speech or extremism, or could trigger or exacerbate mental health problems.\nYouTube has defended its record on removing inappropriate content. \nThe video-sharing site said that 7.8m videos were taken down between July and September 2018, with 81% of them automatically removed by machines, and three-quarters of those clips never receiving a single view.  \nGlobally, YouTube employs 10,000 people in monitoring and removing content, as well as policy development.\nFacebook, which owns Instagram, told the BBC that it has 30,000 people around the world working on safety and security. It said that it removed 15.4m pieces of violent content between October and December, up from 7.9m in the previous three months.\nSome content can be automatically detected and removed before it is seen by users. In the case of terrorist propaganda, Facebook says 99.5% of all the material taken down between July and September was done by \"detection technology\".\nIf illegal content, such as \"revenge pornography\" or extremist material, is posted on a social media site, it will be the person who posted it, rather than the social media companies, who is most at risk of prosecution. \nThis is a situation that needs to change, according to Culture Minister Margot James. She wants the government to bring in legislation that will force social media platforms to remove illegal content and \"prioritise the protection of users, especially children, young people and vulnerable adults\".\nSo if the UK has so far mainly relied on social media platforms governing themselves, what do other countries do? \nGermany's NetzDG law came into effect at the beginning of 2018, applying to companies with more than two million registered users in the country.\nThey were forced to set up procedures to review complaints about content they are hosting and remove anything that is clearly illegal within 24 hours.  \nIndividuals may be fined up to \u20ac5m ($5.6m; \u00a34.4m) and companies up to \u20ac50m for failing to comply with these requirements. \nIn the first year of the new law there were reported to have been 714 complaints from users who said that online platforms had not deleted or blocked illegal content within the statutory period.\nThe Federal Ministry of Justice confirmed to the BBC that the figure had been considerably below the 25,000 complaints a year it had been expecting and that there have been no fines issued so far.\nThe EU is considering a clampdown, specifically on terror videos.\nSocial media platforms would face fines if they did not delete extremist content within an hour.\nThe EU also introduced the General Data Protection Regulation (GDPR) which set rules on how companies, including social media platforms, store and use people's data.\nBut it is another proposed directive that has worried internet companies. \nArticle 13 of the copyright directive would put the responsibility on platforms to make sure that copyright infringing content is not hosted on their sites.\nPrevious legislation has only required the platforms to take down such content if it is pointed out to them. Shifting the responsibility would be a big deal for social media companies.  \nAustralia passed the Sharing of Abhorrent Violent Material Act on 5 April, introducing criminal penalties for social media companies, possible jail sentences for tech executives for up to three years and financial penalties worth up to 10% of a company's global turnover.\nIt followed the live-streaming of the New Zealand shootings on Facebook.\nIn 2015, the Enhancing Online Safety Act created an eSafety Commissioner with the power to demand that social media companies take down harassing or abusive posts. Last year, the powers were expanded to include revenge porn. \nThe eSafety Commissioner's office can issue companies with 48-hour \"takedown notices\", and fines of up to 525,000 Australian dollars (\u00a3285,000). But it can also fine individuals up to A$105,000 for posting the content. \nThe legislation was introduced after the death of Charlotte Dawson, a TV presenter and a judge on Australia's Next Top Model, who killed herself in 2014 following a campaign of cyber-bullying against her on Twitter. She had a long history of depression. \nUnder Russia's data laws from 2015, social media companies are required to store any data about Russians on servers within the country. \nIts communications watchdog is taking action against Facebook and Twitter for not being clear about how they planned to comply with this.\nRussia is also considering two laws similar to Germany's example, requiring platforms to take down offensive material within 24 hours of being alerted to it and imposing fines on companies that fail to do so.\nSites such as Twitter, Google and WhatsApp are blocked in China. Their services are provided instead by Chinese applications such as Weibo, Baidu and WeChat.\nChinese authorities have also had some success in restricting access to the virtual private networks that some users have employed to bypass the blocks on sites.\nThe Cyberspace Administration of China announced at the end of January that in the previous six months it had closed 733 websites and \"cleaned up\" 9,382 mobile apps, although those are more likely to be illegal gambling apps or copies of existing apps being used for illegal purposes than social media.\nChina has hundreds of thousands of cyber-police, who monitor social media platforms and screen messages that are deemed to be politically sensitive. \nSome keywords are automatically censored outright, such as references to the 1989 Tiananmen Square incident.\nNew words that are seen as being sensitive are added to a long list of censored words and are either temporarily banned, or are filtered out from social platforms. \nWhat do you want BBC Reality Check to investigate? Get in touch\nRead more from Reality Check\nFollow us on Twitter", "date": "8 April 2019", "href": "https://www.bbc.com/news/technology-47135058", "tags": "users platforms media fines within companies regulate social facebook content illegal governments"}