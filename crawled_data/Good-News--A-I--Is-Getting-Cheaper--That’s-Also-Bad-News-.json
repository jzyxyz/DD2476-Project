{"title": "Good News: A.I. Is Getting Cheaper. That\u2019s Also Bad News.", "content": "SAN FRANCISCO \u2014 A Silicon Valley start-up recently unveiled a drone that can set a course entirely on its own. A handy smartphone app allows the user to tell the airborne drone to follow someone. Once the drone starts tracking, its subject will find it remarkably hard to shake.\nThe drone is meant to be a fun gadget \u2014 sort of a flying selfie stick. But it is not unreasonable to find this automated bloodhound a little unnerving.\nOn Tuesday, a group of artificial intelligence researchers and policymakers from prominent labs and think tanks in both the United States and Britain released a report that described how rapidly evolving and increasingly affordable A.I. technologies could be used for malicious purposes. They proposed preventive measures including being careful with how research is shared: Don\u2019t spread it widely until you have a good understanding of its risks.\nA.I. experts and pundits have discussed the threats created by the technology for years, but this is among the first efforts to tackle the issue head-on. And the little tracking drone helps explain what they are worried about.\nThe drone, made by a company called Skydio and announced this month, costs $2,499. It was made with technological building blocks that are available to anyone: ordinary cameras, open-source software and low-cost computer chips.\nIn time, putting these pieces together \u2014 researchers call them dual-use technologies \u2014 will become increasingly easy and inexpensive. How hard would it be to make a similar but dangerous device?\n\u201cThis stuff is getting more available in every sense,\u201d said one of Skydio\u2019s founders, Adam Bry. These same technologies are bringing a new level of autonomy to cars, warehouse robots, security cameras and a wide range of internet services.\nBut at times, new A.I. systems also exhibit strange and unexpected behavior because the way they learn from large amounts of data is not entirely understood. That makes them vulnerable to manipulation; today\u2019s computer vision algorithms, for example, can be fooled into seeing things that are not there.\n\u201cThis becomes a problem as these systems are widely deployed,\u201d said Miles Brundage, a research fellow at the University of Oxford\u2019s Future of Humanity Institute and one of the report\u2019s primary authors. \u201cIt is something the community needs to get ahead of.\u201d\nThe report warns against the misuse of drones and other autonomous robots. But there may be bigger concerns in less obvious places, said Paul Scharre, another author of the report, who had helped set policy involving autonomous systems and emerging weapons technologies at the Defense Department and is now a senior fellow at the Center for a New American Security.\n\u201cDrones have really captured the imagination,\u201d he said. \u201cBut what is harder to anticipate \u2014 and wrap our heads around \u2014 is all the less tangible ways that A.I. is being integrated into our lives.\u201d\nThe rapid evolution of A.I. is creating new security holes. If a computer-vision system can be fooled into seeing things that are not there, for example, miscreants can circumvent security cameras or compromise a driverless car.\nResearchers are also developing A.I. systems that can find and exploit security holes in all sorts of other systems, Mr. Scharre said. These systems can be used for both defense and offense.\nAutomated techniques will make it easier to carry out attacks that now require extensive human labor, including \u201cspear phishing,\u201d which involves gathering and exploiting personal data of victims. In the years to come, the report said, machines will be more adept at collecting and deploying this data on their own.\nA.I. systems are increasingly adept at generating believable audio and video on their own. This will accelerate the progress of virtual reality, online games and movie animation. It will also make it easier for bad actors to spread misinformation online, the report said.\nThis is already beginning to happen through a technology called \u201cDeepfakes,\u201d which provides a simple way of grafting anyone\u2019s head onto a pornographic video \u2014 or put words into the mouth of the president.\nSome believe concerns over the progress of A.I. are overblown. Alex Dalyac, chief executive and co-founder of a computer vision start-up called Tractable, acknowledged that machine learning will soon produce fake audio and video that humans cannot distinguish from the real thing. But he believes other systems will also get better at identifying misinformation. Ultimately, he said, these systems will win the day.\nTo others, that sounds like an endless cat-and-mouse game between A.I. systems trying to create the fake content and those trying to identify it.\n\u201cWe need to assume that there will be advances on both sides,\u201d Mr. Scharre said.", "date": "Feb. 20, 2018", "href": "https://www.nytimes.com/2018/02/20/technology/artificial-intelligence-risks.html", "tags": "security \u201c news report drone good a.i technologies systems bad also cheaper getting \u2014"}