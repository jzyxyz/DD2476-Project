{"title": "Facial Recognition Is Accurate, if You\u2019re a White Guy", "content": "Facial recognition technology is improving by leaps and bounds. Some commercial software can now tell the gender of a person in a photograph.\nWhen the person in the photo is a white man, the software is right 99 percent of the time.\nBut the darker the skin, the more errors arise \u2014 up to nearly 35 percent for images of darker skinned women, according to a new study that breaks fresh ground by measuring how the technology works on people of different races and gender.\nThese disparate results, calculated by Joy Buolamwini, a researcher at the M.I.T. Media Lab, show how some of the biases in the real world can seep into artificial intelligence, the computer systems that inform facial recognition.\nIn modern artificial intelligence, data rules. A.I. software is only as smart as the data used to train it. If there are many more white men than black women in the system, it will be worse at identifying the black women.\nOne widely used facial-recognition data set was estimated to be more than 75 percent male and more than 80 percent white, according to another research study.\nThe new study also raises broader questions of fairness and accountability in artificial intelligence at a time when investment in and adoption of the technology is racing ahead.\nToday, facial recognition software is being deployed by companies in various ways, including to help target product pitches based on social media profile pictures. But companies are also experimenting with face identification and other A.I. technology as an ingredient in automated decisions with higher stakes like hiring and lending.\nResearchers at the Georgetown Law School estimated that 117 million American adults are in face recognition networks used by law enforcement \u2014 and that African Americans were most likely to be singled out, because they were disproportionately represented in mug-shot databases.\nFacial recognition technology is lightly regulated so far.\n\u201cThis is the right time to be addressing how these A.I. systems work and where they fail \u2014 to make them socially accountable,\u201d said Suresh Venkatasubramanian, a professor of computer science at the University of Utah.\nUntil now, there was anecdotal evidence of computer vision miscues, and occasionally in ways that suggested discrimination. In 2015, for example, Google had to apologize after its image-recognition photo app initially labeled African Americans as \u201cgorillas.\u201d\nSorelle Friedler, a computer scientist at Haverford College and a reviewing editor on Ms. Buolamwini\u2019s research paper, said experts had long suspected that facial recognition software performed differently on different populations.\n\u201cBut this is the first work I\u2019m aware of that shows that empirically,\u201d Ms. Friedler said.\nMs. Buolamwini, a young African-American computer scientist, experienced the bias of facial recognition firsthand. When she was an undergraduate at the Georgia Institute of Technology, programs would work well on her white friends, she said, but not recognize her face at all. She figured it was a flaw that would surely be fixed before long.\nBut a few years later, after joining the M.I.T. Media Lab, she ran into the missing-face problem again. Only when she put on a white mask did the software recognize hers as a face.\nBy then, face recognition software was increasingly moving out of the lab and into the mainstream.\n\u201cO.K., this is serious,\u201d she recalled deciding then. \u201cTime to do something.\u201d\nSo she turned her attention to fighting the bias built into digital technology. Now 28 and a doctoral student, after studying as a Rhodes scholar and a Fulbright fellow, she is an advocate in the new field of \u201calgorithmic accountability,\u201d which seeks to make automated decisions more transparent, explainable and fair.\nHer short TED Talk on coded bias has been viewed more than 940,000 times, and she founded the Algorithmic Justice League, a project to raise awareness of the issue.\nIn her newly published paper, which will be presented at a conference this month, Ms. Buolamwini studied the performance of three leading face recognition systems \u2014 by Microsoft, IBM and Megvii of China \u2014 by classifying how well they could guess the gender of people with different skin tones. These companies were selected because they offered gender classification features in their facial analysis software \u2014 and their code was publicly available for testing.\nShe found them all wanting.\nTo test the commercial systems, Ms. Buolamwini built a data set of 1,270 faces, using faces of lawmakers from countries with a high percentage of women in office. The sources included three African nations with predominantly dark-skinned populations, and three Nordic countries with mainly light-skinned residents.\nThe African and Nordic faces were scored according to a six-point labeling system used by dermatologists to classify skin types. The medical classifications were determined to be more objective and precise than race.\nThen, each company\u2019s software was tested on the curated data, crafted for gender balance and a range of skin tones. The results varied somewhat. Microsoft\u2019s error rate for darker-skinned women was 21 percent, while IBM\u2019s and Megvii\u2019s rates were nearly 35 percent. They all had error rates below 1 percent for light-skinned males.\nMs. Buolamwini shared the research results with each of the companies. IBM said in a statement to her that the company had steadily improved its facial analysis software and was \u201cdeeply committed\u201d to \u201cunbiased\u201d and \u201ctransparent\u201d services. This month, the company said, it will roll out an improved service with a nearly 10-fold increase in accuracy on darker-skinned women.\nMicrosoft said that it had \u201calready taken steps to improve the accuracy of our facial recognition technology\u201d and that it was investing in research \u201cto recognize, understand and remove bias.\u201d\nMs. Buolamwini\u2019s co-author on her paper is Timnit Gebru, who described her role as an adviser. Ms. Gebru is a scientist at Microsoft Research, working on its Fairness Accountability Transparency and Ethics in A.I. group.\nMegvii, whose Face++ software is widely used for identification in online payment and ride-sharing services in China, did not reply to several requests for comment, Ms. Buolamwini said.\nMs. Buolamwini is releasing her data set for others to build upon. She describes her research as \u201ca starting point, very much a first step\u201d toward solutions.\nMs. Buolamwini is taking further steps in the technical community and beyond. She is working with the Institute of Electrical and Electronics Engineers, a large professional organization in computing, to set up a group to create standards for accountability and transparency in facial analysis software.\nShe meets regularly with other academics, public policy groups and philanthropies that are concerned about the impact of artificial intelligence. Darren Walker, president of the Ford Foundation, said that the new technology could be a \u201cplatform for opportunity,\u201d but that it would not happen if it replicated and amplified bias and discrimination of the past.\n\u201cThere is a battle going on for fairness, inclusion and justice in the digital world,\u201d Mr. Walker said.\nPart of the challenge, scientists say, is that there is so little diversity within the A.I. community.\n\u201cWe\u2019d have a lot more introspection and accountability in the field of A.I. if we had more people like Joy,\u201d said Cathy O\u2019Neil, a data scientist and author of \u201cWeapons of Math Destruction.\u201d\nTechnology, Ms. Buolamwini said, should be more attuned to the people who use it and the people it\u2019s used on.\n\u201cYou can\u2019t have ethical A.I. that\u2019s not inclusive,\u201d she said. \u201cAnd whoever is creating the technology is setting the standards.\u201d", "date": "Feb. 9, 2018", "href": "https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html", "tags": "accurate software percent facial white recognition \u201d guy buolamwini ms. technology"}