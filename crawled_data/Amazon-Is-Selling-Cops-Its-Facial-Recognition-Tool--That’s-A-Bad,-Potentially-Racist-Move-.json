{"title": "Amazon Is Selling Cops Its Facial Recognition Tool- That\u2019s A Bad, Potentially Racist Move-", "content": "Law enforcement agencies are using Rekognition, Amazon\u2019s facial recognition system, to identify people and track them in real time. This isn\u2019t a case of an outside party making opportunistic use of an emerging technology.\u00a0The American Civil Liberties Union of Northern California\u00a0recently obtained documents that show just how closely Amazon is working with cops around the country to implement its product.\nIn one email, an account manager for Amazon Web Services eagerly offered up his or her services to a Washington County, Oregon, employee: \u201cI am the Account Manager for AWS covering Oregon, and I noticed that you were leveraging our new Rekognition service. Because the service is so new, we are reaching out to customers to make sure they get all the support they need to succeed with their particular use case.\u201d\nAs the people \u201cleveraging\u201d the Rekognition technology may or may not realize, facial recognition software tends to be racist. \nThe artificial intelligence-powered system can analyze faces and almost immediately run them through larger databases featuring tens of millions of faces to produce a similar result.\u00a0Amazon sees police use of its tool as \u201ccommon\u201d\u00a0and describes the technology as an \u201ceasy and accurate way\u201d to monitor public spaces and identify \u201cpeople of interest.\u201d Law enforcement agencies in Orlando, Florida, and Washington County have praised the software.\nOrlando has the ability to identify faces in real-time via a network of cameras located across the city. Washington County has taken at least 300,000 mugshots and built a database to use with Rekognition.\nBut the technology is fallible. It often does not properly identify darker-skinned people. The Rekogniton software can identify \u201call faces in group photos, crowded events, and public places such as airports.\u201d It can also recognize up to 100 people in a single image. Such broad capabilities, when coupled with balky technology, could lead to the wrong people being targeted during investigations. \nResearch from Joy Buolamwini of M.I.T. Media Lab found that three AI-powered facial recognition systems \u00a0\u2014 Microsoft, IBM, and Face++ \u2014 identified men more accurately than women and performed better on lighter-skinned faces than those with dark skin despite having fairly high accuracy overall.\nA 2016 study from Georgetown Law\u2019s Center on Privacy and Technology found that over 117 million American adults are included in facial recognition databases used by law enforcement agencies. The databases have a disproportionate effect on African-Americans, said Clare Garvie, one of the lead researchers, at the time. The software less accurately identifies black people even though they are overrepresented in the databases.\n\u201cIf police are looking for an African-American suspect, they may miss even if that person is in their database \u2014 it may not find that person,\u201d she said. \u201cBut these systems are not designed to give no for an answer. They\u2019re designed to give a list of possible matches. So if they don\u2019t find the right person, they provide a list of the wrong people \u2014 and that will happen more with African-Americans.\u201d\nAcademic research has not been conducted on the accuracy of Amazon\u2019s product, but history tells us what happens when black people are thrown into the panopticon. Local police and the federal government have a history of surveilling social movements \u2015 most notably COINTELPRO, a civil rights era ploy on the part of the FBI to stifle progressive organizations and black social movements.\nIn an August 2017 report, the FBI claimed that \u201cblack identity extremists\u201d concerned with \u201calleged police brutality\u201d were likely to take up arms against law enforcement. The Los Angeles Police already monitors the \u201csuspicious activities\u201d of civilians through the Nationwide Suspicious Activity Reporting Initiative. An inspector general\u2019s audit of the program in January 2015 found that more than 30 percent of the reports involved black residents despite black people making up only 9.6 percent of the city\u2019s population. Baltimore\u2019s police department tracks the cell phone use of residents and films their movements using drones.\nBorder Patrol has expressed a desire to build face-recognizing drones and have tested facial scans at airports to track undocumented immigrants who may have overstayed their visas. (Just over 1 percent of all travelers overstayed their visas in 2016.) And former Congressman Jason Chaffetz (R-Utah) also proposed using the software to track undocumented immigrants.\n\u201cWe already know that facial recognition algorithms discriminate against Black faces, and are being used to violate the human rights of immigrants,\u201d said Malkia Cyril, executive director of the Center for Media Justice, in a press release. \n\u201cWe know that putting this technology into the hands of already brutal and unaccountable law enforcement agencies places both democracy and dissidence at great risk,\u201d she added. \u201cAmazon should never be in the business of aiding and abetting racial discrimination and xenophobia \u2014 but that\u2019s exactly what Amazon CEO Jeff Bezos is doing when he sells these loosely regulated facial recognition tools to local police departments.\u201d", "date": "\n05/22/2018", "href": "https://www.huffpost.com/entry/amazon-is-selling-cops-its-facial-recognition-tool-thats-a-bad-potentially-racist-move_n_5b0478fce4b05f0fc84289ae", "tags": "technology facial law recognition \u201d potentially racist cops move- selling black bad police tool- amazon"}