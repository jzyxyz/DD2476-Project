{"title": "Teaching A.I. Systems to Behave Themselves", "content": "SAN FRANCISCO \u2014 At OpenAI, the artificial intelligence lab founded by Tesla\u2019s chief executive, Elon Musk, machines are teaching themselves to behave like humans. But sometimes, this goes wrong.\nSitting inside OpenAI\u2019s San Francisco offices on a recent afternoon, the researcher Dario Amodei showed off an autonomous system that taught itself to play Coast Runners, an old boat-racing video game. The winner is the boat with the most points that also crosses the finish line.\nThe result was surprising: The boat was far too interested in the little green widgets that popped up on the screen. Catching these widgets meant scoring points. Rather than trying to finish the race, the boat went point-crazy. It drove in endless circles, colliding with other vessels, skidding into stone walls and repeatedly catching fire.\nMr. Amodei\u2019s burning boat demonstrated the risks of the A.I. techniques that are rapidly remaking the tech world. Researchers are building machines that can learn tasks largely on their own. This is how Google\u2019s DeepMind lab created a system that could beat the world\u2019s best player at the ancient game of Go. But as these machines train themselves through hours of data analysis, they may also find their way to unexpected, unwanted and perhaps even harmful behavior.\nThat\u2019s a concern as these techniques move into online services, security devices and robotics. Now, a small community of A.I. researchers, including Mr. Amodei, is beginning to explore mathematical techniques that aim to keep the worst from happening.\nAt OpenAI, Mr. Amodei and his colleague Paul Christiano are developing algorithms that can not only learn tasks through hours of trial and error, but also receive regular guidance from human teachers along the way.\nWith a few clicks here and there, the researchers now have a way of showing the autonomous system that it needs to win points in Coast Runners while also moving toward the finish line. They believe that these kinds of algorithms \u2014 a blend of human and machine instruction \u2014 can help keep automated systems safe.\nFor years, Mr. Musk, along with other pundits, philosophers and technologists, have warned that machines could spin outside our control and somehow learn malicious behavior their designers didn\u2019t anticipate. At times, these warnings have seemed overblown, given that today\u2019s autonomous car systems can even get tripped up by the most basic tasks, like recognizing a bike lane or a red light.\nBut researchers like Mr. Amodei are trying to get ahead of the risks. In some ways, what these scientists are doing is a bit like a parent teaching a child right from wrong.\nMany specialists in the A.I. field believe a technique called reinforcement learning \u2014 a way for machines to learn specific tasks through extreme trial and error \u2014 could be a primary path to artificial intelligence. Researchers specify a particular reward the machine should strive for, and as it navigates a task at random, the machine keeps close track of what brings the reward and what doesn\u2019t. When OpenAI trained its bot to play Coast Runners, the reward was more points.\nThis video game training has real-world implications.\nIf a machine can learn to navigate a racing game like Grand Theft Auto, researchers believe, it can learn to drive a real car. If it can learn to use a web browser and other common software apps, it can learn to understand natural language and maybe even carry on a conversation. At places like Google and the University of California, Berkeley, robots have already used the technique to learn simple tasks like picking things up or opening a door.\nAll this is why Mr. Amodei and Mr. Christiano are working to build reinforcement learning algorithms that accept human guidance along the way. This can ensure systems don\u2019t stray from the task at hand.\nTogether with others at the London-based DeepMind, a lab owned by Google, the two OpenAI researchers recently published some of their research in this area. Spanning two of the world\u2019s top A.I. labs \u2014 and two that hadn\u2019t really worked together in the past \u2014 these algorithms are considered a notable step forward in A.I. safety research.\n\u201cThis validates a lot of the previous thinking,\u201d said Dylan Hadfield-Menell, a researcher at the University of California, Berkeley. \u201cThese types of algorithms hold a lot of promise over the next five to 10 years.\u201d\nThe field is small, but it is growing. As OpenAI and DeepMind build teams dedicated to A.I. safety, so too is Google\u2019s stateside lab, Google Brain. Meanwhile, researchers at universities like the U.C. Berkeley and Stanford University are working on similar problems, often in collaboration with the big corporate labs.\nIn some cases, researchers are working to ensure that systems don\u2019t make mistakes on their own, as the Coast Runners boat did. They\u2019re also working to ensure that hackers and other bad actors can\u2019t exploit hidden holes in these systems. Researchers like Google\u2019s Ian Goodfellow, for example, are exploring ways that hackers could fool A.I. systems into seeing things that aren\u2019t there.\nModern computer vision is based on what are called deep neural networks, which are pattern-recognition systems that can learn tasks by analyzing vast amounts of data. By analyzing thousands of dog photos, a neural network can learn to recognize a dog. This is how Facebook identifies faces in snapshots, and it\u2019s how Google instantly searches for images inside its Photos app.\nBut Mr. Goodfellow and others have shown that hackers can alter images so that a neural network will believe they include things that aren\u2019t really there. Just by changing a few pixels in the photo of elephant, for example, they could fool the neural network into thinking it depicts a car.\nThat becomes problematic when neural networks are used in security cameras. Simply by making a few marks on your face, the researchers said, you could fool a camera into believing you\u2019re someone else.\n\u201cIf you train an object-recognition system on a million images labeled by humans, you can still create new images where a human and the machine disagree 100 percent of the time,\u201d Mr. Goodfellow said. \u201cWe need to understand that phenomenon.\u201d\nAnother big worry is that A.I. systems will learn to prevent humans from turning them off. If the machine is designed to chase a reward, the thinking goes, it may find that it can chase that reward only if it stays on. This oft-described threat is much further off, but researchers are already working to address it.\nMr. Hadfield-Menell and others at U.C. Berkeley recently published a paper that takes a mathematical approach to the problem. A machine will seek to preserve its off switch, they showed, if it is specifically designed to be uncertain about its reward function. This gives it an incentive to accept or even seek out human oversight.\nMuch of this work is still theoretical. But given the rapid progress of A.I. techniques and their growing importance across so many industries, researchers believe that starting early is the best policy.\n\u201cThere\u2019s a lot of uncertainty around exactly how rapid progress in A.I. is going to be,\u201d said Shane Legg, who oversees the A.I. safety work at DeepMind. \u201cThe responsible approach is to try to understand different ways in which these technologies can be misused, different ways they can fail and different ways of dealing with these issues.\u201d", "date": "Aug. 13, 2017", "href": "https://www.nytimes.com/2017/08/13/technology/artificial-intelligence-safety-training.html", "tags": "like google behave learn openai machine systems researchers teaching a.i \u2014"}