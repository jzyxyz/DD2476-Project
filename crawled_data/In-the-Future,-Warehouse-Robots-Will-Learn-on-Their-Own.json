{"title": "In the Future, Warehouse Robots Will Learn on Their Own", "content": "BERKELEY, Calif. \u2014 The robot was perched over a bin filled with random objects, from a box of instant oatmeal to a small toy shark. This two-armed automaton did not recognize any of this stuff, but that did not matter. It reached into the pile and started picking things up, one after another after another.\n\u201cIt figures out the best way to grab each object, right from the middle of the clutter,\u201d said Jeff Mahler, one of the researchers developing the robot inside a lab at the University of California, Berkeley.\nFor the typical human, that is an easy task. For a robot, it is a remarkable talent \u2014 something that could drive significant changes inside some of the world\u2019s biggest businesses and further shift the market for human labor.\nToday, robots play important roles inside retail giants like Amazon and manufacturing companies like Foxconn. But these machines are programmed for very specific tasks, like moving a particular type of container across a warehouse or placing a particular chip on a circuit board. They can\u2019t sort through a big pile of stuff, or accomplish more complex tasks. Inside Amazon\u2019s massive distribution centers \u2014 where sorting through stuff is the primary task \u2014 armies of humans still do most of the work.\nThe Berkeley robot was all the more remarkable because it could grab stuff it had never seen before. Mr. Mahler and the rest of the Berkeley team trained the machine by showing it hundreds of purely digital objects, and after that training, it could pick up items that weren\u2019t represented in its digital data set.\n\u201cWe\u2019re learning from simulated models and then applying that to real work,\u201d said Ken Goldberg, the Berkeley professor who oversees the university\u2019s automation lab.\nThe robot was far from perfect, and it could be several years before it is seen outside research labs. Though it was equipped with a suction cup or a parallel gripper \u2014 a kind of two-fingered hand \u2014 it could reliably handle only so many items. And it could not switch between the cup and the gripper on the fly. But the techniques used to train it represented a fundamental shift in robotics research, a shift that could overhaul not just Amazon\u2019s warehouses but entire industries.\nRather than trying to program behavior into their robot \u2014 a painstaking task \u2014 Mr. Mahler and his team gave it a way of learning tasks on its own. Researchers at places like Northeastern University, Carnegie Mellon University, Google and OpenAI \u2014 the artificial intelligence lab founded by Tesla\u2019s chief executive, Elon Musk \u2014 are developing similar techniques, and many believe that such machine learning will ultimately allow robots to master a much wider array of tasks, including manufacturing.\n\u201cThis can extend to tasks of assembly and more complex operations,\u201d said Juan Aparicio, head of advanced manufacturing automation at the German industrial giant Siemens, which is helping to fund the research at Berkeley. \u201cThat is the road map.\u201d\nPhysically, the Berkeley robot was nothing new. Mr. Mahler and his team were using existing hardware, including two robotic arms from the Swiss multinational ABB and a camera that captured depth.\nWhat was different was the software. It demonstrated a new use for what are called neural networks. Loosely based on the network of neurons in the human brain, a neural network is a complex algorithm that can learn tasks by analyzing vast amounts of data. By looking for patterns in thousands of dog photos, for instance, a neural network can learn to recognize a dog.\nOver the past five years, these algorithms have radically changed the way the internet\u2019s largest companies build their online services, accelerating the development of everything from image and speech recognition to internet search. But they can also accelerate the development of robotics.\nThe Berkeley team began by scouring the internet for CAD models, short for computer-aided design. These are digital representations of physical objects. Engineers, physicists and designers build them when running experiments or creating new products. Using these models, Mr. Mahler and his team generated many more digital objects, eventually building a database of more than seven million items. Then they simulated the physics of each item, showing the precise point where a robotic arm should pick it up.\nThat was a large task, but the process was mostly automated. When the team fed these models into a neural network, it learned to identify a similar point on potentially any digital object with any shape. And when the team plugged this neural network into the two-armed robot, it could do the same with physical objects.\nWhen facing a single everyday object with cylindrical or at least partly planar surfaces \u2014 like a spatula, a stapler, a cylindrical container of Froot Loops or even a tube of toothpaste \u2014 it could typically pick it up, with success rates often above 90 percent. But percentages dropped with more complex shapes, like the toy shark.\nWhat\u2019s more, when the team built simulated piles of random objects and fed those into the neural network, it could learn to lift items from physical piles, too. Researchers at Brown University and Northeastern are exploring similar research, and the hope is that this kind of work can be combined with other methods.\nLike Siemens and the Toyota Research Institute, Amazon is helping to fund the work at Berkeley, and it has an acute need for this kind of robot. For the past three years, the company has run a contest in which researchers seek to solve the \u201cpick and place\u201d problem. But the promise of machine-learning methods like the one used at Berkeley is that they can eventually extend to so many other areas, including manufacturing and home robotics.\n\u201cPicking an object up is the first thing you want a manipulator robot to do,\u201d said Stefanie Tellex, a professor at Brown. \u201cA lot of more sophisticated behavior begins with that. If you can\u2019t pick it up, game over.\u201d\nThe research demonstrated how a task learned in the digital world can be transferred to the physical. Since the camera on Berkeley\u2019s robot could see depth, it captured three-dimensional images that were not unlike the CAD models the team uses to train its neural network.\nOther researchers are developing ways for robots to learn directly from physical experience. For example, at Google, using an algorithmic technique called reinforcement learning, robots are training themselves to open doors through extreme trial and error. But this kind of physical training is both time consuming and expensive. Digital training is more efficient.\nFor this reason, some organizations are hoping to train robots using complex virtual worlds \u2014 digital recreations of our physical environment. If a system can train itself to navigate a car racing game like Grand Theft Auto, the thinking goes, it can navigate real roads.\nThis is still largely theory. But at places like Berkeley and Northeastern, researchers are showing that digital learning can indeed make the leap into the real world.\n\u201cThis is a challenge,\u201d said Rob Platt, a professor at Northeastern. \u201cBut it\u2019s a challenge we\u2019re dealing with.\u201d", "date": "Sept. 10, 2017", "href": "https://www.nytimes.com/2017/09/10/business/warehouse-robots-learning.html", "tags": "like berkeley could warehouse team learn \u201d robots future neural robot \u2014 digital"}